# Run Analysis

This directory contains scripts for analyzing the performance of different regex composition strategies across multiple evaluation dimensions.

## Overview

The analysis pipeline processes regex candidates generated by various composition strategies (reuse-by-example, formal synthesizers, and LLMs) and evaluates them along four key dimensions:

1. **Accuracy**: How well regex candidates match the intended patterns
2. **Syntactic Similarity**: How structurally similar regex candidates are to ground truth regexes
3. **Semantic Similarity**: How functionally similar regex candidates are to ground truth regexes
4. **Helpfulness Score**: How conservative or liberal regex candidates are compared to the developer's intended solution

## Scripts

### analyze_accuracies.py

Calculates the accuracy of regex candidates by testing them against positive and negative example strings. The script:
- Processes both full-match and partial-match test cases
- Outputs accuracy scores for each candidate regex
- We are running accuracy analysis for all strategies (i.e., not just the top performing ones)

### analyze_syntactic_similarities.py

Measures the structural similarity between regex candidates and ground truth regexes using AST (Abstract Syntax Tree) edit distance. The script:
- Uses the Zhang-Shasha tree edit distance algorithm from the `regex_syntactic_sim` module
- Reports both raw and normalized edit distances
- Targets representative strategies (reuse-by-example, RFixer, o3-mini) for in-depth analysis

### analyze_semantic_similarities.py

Evaluates the functional similarity between regex candidates and ground truth regexes by comparing their matching behavior. The script:
- Uses the `regex_semantic_sim` module
- Generates test strings for each regex and compares accept/reject decisions
- Computes a symmetrical (bidirectional) semantic similarity measure

### analyze_helpfulness_score.py

Calculates the helpfulness score metric that quantifies how conservative or liberal a regex candidate is relative to the minimum necessary pattern. The script:
- Uses the `helpfulness_score` module
- Distinguishes between regexes that are too specific (conservative) or too general (liberal)
- Helps evaluate how well each strategy provides diverse constraint options

## Usage

Each script processes data in a pipeline fashion, where the output of one stage becomes the input for the next. To run the complete analysis pipeline:

1. Calculate accuracy for all regex candidates:
```bash
python analyze_accuracies.py
```

2. Calculate syntactic similarity for top performing candidates:
```bash
python analyze_syntactic_similarities.py
```

3. Calculate semantic similarity for top performing candidates:
```bash
python analyze_semantic_similarities.py
```

4. Calculate helpfulness scores for top performing candidates:
```bash
python analyze_helpfulness_score.py
```

## Data Flow

The pipeline follows this data flow:
```
generated-regexes/
    ├─> generated-regexes-with-accuracies/
    │       ├─> generated-regexes-with-syntactic-similarities/
    │       │       ├─> generated-regexes-with-semantic-similarities/
    │       │       │       └─> generated-regexes-with-helpfulness-scores/
```

Each step in the pipeline enriches the data with additional metrics.

## Configuration

The scripts can be configured by modifying those within the script itself:
- `BASE_DIR`: The root directory for input/output data
- `INPUT_DIR`: Source directory for the current stage
- `OUTPUT_DIR`: Target directory for processed results
- `BEST_TARGETS`: Specific files to focus analysis on (for optimizing computational resources)
- `n_chunks`: Number of parallel processes to use (defaults to CPU count)

## Requirements

- Python 3.9 or higher
- Install required Python packages with `pip install -r requirements.txt`
- Custom modules:
  - `helpfulness_score`
  - `regex_semantic_sim`
  - `regex_syntactic_sim`
